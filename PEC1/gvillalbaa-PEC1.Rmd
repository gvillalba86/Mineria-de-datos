---
title: 'Minería de datos: PEC1'
author: "Autor: Gerson Villalba Arana"
date: "octubre 2021"
output:
  pdf_document:
    highlight: zenburn
    toc: yes
  word_document: default
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    includes:
      in_header: 75.584-PEC-header.html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

****
# Introducción
****
## Presentación
Esta prueba de evaluación continuada cubre los módulos "El proceso de minería de datos" y "Preprocesado de los datos y gestión de características" del programa de la asignatura.

## Objetivos
* Asimilar correctamente los módulos citados.
* Qué es y que no es MD.
* Ciclo de vida de los proyectos de MD.
* Diferentes tipologías de MD.
* Conocer las técnicas propias de una fase de conocimiento, preparación de datos y objetivos a lograr.

## Descripción de la PEC a realizar
La prueba está estructurada en 1 ejercicio teórico/práctico y 1 ejercicio práctico que pide que se desarrolle la fase de conocimiento y preparación con un juego de datos.
Se tienen que responderse todos los ejercicios para poder superar la PEC.
La PEC está pensada para resolverla en el entorno Markdown con RStudio con R como lenguaje preferido. Se recomienda hacerlo así. Si tenéis las competencias para hacerlo en Python no hay ningún problema. Podéis hacerlo. Simplemente sustituis los chunks de R por chunks en Python. 

## Recursos
Para realizar esta práctica recomendamos como punto de partida la lectura de los siguientes documentos:

* Los módulos "El proceso de minería de datos" y "Preprocesado de los datos y gestión de características" del programa de la asignatura.
* Ciclo de vida de un proyecto de minería de datos: https://es.wikipedia.org/wiki/cross_industry_standard_process_for_data_mining#Fases_principales
* Al apartado del enunciado de la actividad disponéis de unos materiales de ggplot2
* El aula laboratorio de R para resolver dudas o problemas.
* RStudio Cheat Sheet: Disponible en el aula Laboratorio de Minería de datos.
* R Base Cheat Sheet: Disponible en el aula Laboratorio de Minería de datos.


## Formato y fecha de entrega
El formato de entrega es: **usernameestudiante-PEC1.html (pdf o word) y rmd**. 
Fecha de Entrega: 27/10/2021.
Se tiene que librar la PEC en el buzón de entregas del aula.


## Nota: Propiedad intelectual

A menudo es inevitable, al producir una obra multimedia, hacer uso de recursos creados por terceras personas. Es por lo tanto comprensible hacerlo en el marco de una práctica de los estudios de Informática, Multimedia y Telecomunicación de la UOC, siempre que esto se documente claramente y no suponga plagio en la práctica.

Por lo tanto, al presentar una práctica que haga uso de recursos ajenos, se tiene que presentar junto con ella un documento en que se detallen todos ellos, especificando el nombre de cada recurso, su autor, el lugar donde se obtuvo y su estatus legal: si la obra está protegida por el copyright o se acoge a alguna otra licencia de uso (Creative Commons, licencia GNU, GPL ...).
El estudiante tendrá que asegurarse que la licencia no impide específicamente su uso en el marco de la práctica. En caso de no encontrar la información correspondiente tendrá que asumir que la obra está protegida por copyright.

Habréis, además, adjuntar los ficheros originales cuando las obras utilizadas sean digitales, y su código fuente si corresponde.

****
# Ejemplo de solución mínimo del ejercicio 2
****
****
## Objetivos
****
Como muestra, trabajaremos con el juego de datos "Titanic.csv" que recoge datos sobre el famoso crucero.

Las actividades que llevaremos a cabo en esta práctica se hacen en las fases iniciales de un proyecto de minería de datos. Tienen como objetivo obtener un dominio de los datos con las que construiremos el modelo de minería. Tenemos que conocer profundamente los datos tanto en su formato como contenido. Tareas típicas pueden ser la selección de características o variables, la preparación del juego de datos para posteriormente ser consumido por un algoritmo e intentar extraer el máximo conocimiento posible de los datos. Desarrollaremos un subconjunto de tareas mínimas y de ejemplo. Podemos incluir muchas más y mucho más profundas, como hemos visto en el material docente.



## Procesos iniciales con los datos

Primer contacto con el juego de datos.

Instalamos y cargamos las librerías ggplot2 y dplry.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# https://cran.r-project.org/web/packages/ggplot2/index.html
if (!require('ggplot2')) install.packages('ggplot2'); library('ggplot2')
# https://cran.r-project.org/web/packages/dplyr/index.html
if (!require('dplyr')) install.packages('dplyr'); library('dplyr')
```

Cargamos el fichero de datos.

```{r}
totalData <- read.csv('titanic.csv',stringsAsFactors = FALSE)
filas=dim(totalData)[1]
```

Guardamos los datos filtrados por tripulación para hacer estudios posteriores.

```{r}
totalData_crew=subset(totalData, totalData$class=="engineering crew")
```

Verificamos la estructura del juego de datos principal.

```{r}
str(totalData)
```

Vemos que tenemos 2207 registros que se corresponden a los viajeros y tripulación del Titánic y 11 variables que los caracterizan.

Revisamos la descripción de las variables contenidas al fichero y si los tipos de variable se corresponde al que hemos cargado:

**name**
    string with the name of the passenger.
    
**gender**
    factor with levels male and female.
    
**age**
    numeric value with the persons age on the day of the sinking. The age of babies (under 12 months) is given as a fraction of one year (1/month).
    
**class**
    factor specifying the class for passengers or the type of service aboard for crew members.
    
**embarked**
    factor with the persons place of of embarkment.
    
**country**
    factor with the persons home country.
    
**ticketno**
    numeric value specifying the persons ticket number (NA for crew members).
    
**fare**
    numeric value with the ticket price (NA for crew members, musicians and employees of the shipyard company).
    
**sibsp**
    ordered factor specifying the number if siblings/spouses aboard; adopted from Vanderbild data set.
    
**parch**
    an ordered factor specifying the number of parents/children aboard; adopted from Vanderbild data set.
    
**survived**
    a factor with two levels (no and yes) specifying whether the person has survived the sinking.
    
Vamos ahora a sacar estadísticas básicas y después trabajamos los atributos con valores vacíos.

```{r echo=TRUE, message=FALSE, warning=FALSE}
summary(totalData)
```


Estadísticas de valores vacíos.

```{r}
colSums(is.na(totalData))
colSums(totalData=="")
```

Asignamos valor "Desconocido" para los valores vacíos de la variable "country".

```{r}
totalData$country[is.na(totalData$country)] <- "Desconocido"
```

Asignamos la media para valores vacíos de la variable "age".

```{r}
totalData$age[is.na(totalData$age)] <- mean(totalData$age,na.rm=T)
```


De la información mostrada destacamos que el pasajero más joven tenía 6 meses y el más grande 74 años. La media de edad la tenían en 30 años. También podemos ver 891 sin billete. Revisaremos si se corresponde a la tripulación. También podemos observar el que se pagó por el billete. En este caso se entienden las discrepancias en la fiabilidad de este dato. Parece que los pasajeros que embarcaron a Southampton hacían transbordo de un barco que tenía la tripulación en huelga y por eso no tuvieron que pagar lo que explicaría la diferencia. Recordemos que la tripulación no pagaba. Sibsp y parch también muestran datos interesantes el viajero con quien más familiar viajaba eran 8 hermanos o mujer y 9 hijos o paro/madre.

Si observamos los NA (valores nulos) vemos que los datos están bastante bien. Decidimos sustituir el valor NA de country por Desconocido por una mayor legibilidad. También proponemos sustituir los NA de age por la media a pesar de que realmente no hace falta.

Es curios como los valores NA de sibsp y parch nos permite deducir que viajaban muchas familias. De hecho a simple vista, restante la tripulación la gente que viajaba sola era mínima. Este dato la podríamos contrastar también. Sería interesante relacionar la mortalidad del accidente con el tamaño de las familias que viajaban.

Ahora añadiremos un campo nuevo a los datos. Este campos contendrá el valor de la edad discretitzada con un método simple de intervalos de igual amplitud.

```{r echo=TRUE, message=FALSE, warning=FALSE}

summary(totalData[,"age"])
```

Discretizamos con intervalos.

```{r}
totalData["segmento_edad"] <- cut(totalData$age, breaks = c(0,10,20,30,40,50,60,70,100), labels = c("0-9", "10-19", "20-29", "30-39","40-49","50-59","60-69","70-79"))
```

Observamos los datos discretizados.

```{r}
head(totalData)
```

Vemos como se agrupaban por edad.

```{r}
plot(totalData$segmento_edad,main="Número de pasajeros por grupos de edad",xlab="Edad", ylab="Cantidad",col = "ivory")
```

Ahora repetimos por el proceso pero solo por el subconjunto de tripulación filtrado antes.

```{r}
totalData_crew["segmento_edad"] <- cut(totalData_crew$age, breaks = c(0,10,20,30,40,50,60,70,100), labels = c("0-9", "10-19", "20-29", "30-39","40-49","50-59","60-69","70-79"))
plot(totalData_crew$segmento_edad,main="Número de tripulantes por grupos de edad",xlab="Edad", ylab="Cantidad",col = "ivory")
```

De la discretizaón de la edad observamos que realmente la gente que viajaba era muy joven. El segmento más grande era de 20 a 29 años. También vemos de la juventud de la tripulación.


Como alternativa a la discretización realizada discretizaremos ahora edad con kmeans.

```{r}
# https://cran.r-project.org/web/packages/arules/index.html
if (!require('arules')) install.packages('arules'); library('arules')
set.seed(2)
table(discretize(totalData$age, "cluster" ))
hist(totalData$age, main="Número de pasajeros por grupos de edad con kmeans",xlab="Edad", ylab="Cantidad",col = "ivory")
abline(v=discretize(totalData$age, method="cluster", onlycuts=TRUE),col="red")
```

Podemos observar que sin pasar ningún argumento y que el algoritmo  escoja el conjunto de particiones se muestran tres clústeres que agrupan las edades en las franjas mencionadas.
Podemos asignar el propio clúster como una variable más al dataset para trabajar después.


```{r}
totalData$edad_KM <- (discretize(totalData$age, "cluster" ))
head(totalData)
```

Ahora normalizaremos la edad de los pasajeros por el máximo añadiendo un nuevo valor a los datos que contendrá el valor.

```{r}
totalData$age_NM <- (totalData$age/max(totalData[,"age"]))
head(totalData$age_NM)
```

Supongamos que queremos normalizar por la diferencia para ubicar entre 0 y 1 la variable edad del pasajero dado que el algoritmo de minería que utilizaremos así lo requiere. observamos la distribución de la variable original y las tres generadas

```{r}
totalData$age_ND = (totalData$age-min(totalData$age))/(max(totalData$age)-min(totalData$age))

max(totalData$age)
min(totalData$age)
hist(totalData$age,xlab="Edad", col="ivory",ylab="Cantidad", main="Número de pasajeros por grupos de edad")
hist(totalData$age_NM,xlab="Edad normalizada por el máximo", ylab="Cantidad",col="ivory", main="Número de pasajeros por grupos de edad")
hist(totalData$age_ND,xlab="Edad normalizada por la diferencia",ylab="Cantidad", col="ivory", main="Número de pasajeros por grupos de edad")
```

## Procesos de análisis visuales del juego de datos

Nos proponemos analizar las relaciones entre las diferentes variables del juego de datos para ver si se relacionan y como.

Visualizamos la relación entre las variables "gender" y "survived":

```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(data=totalData[1:filas,],aes(x=gender,fill=survived))+geom_bar()+ggtitle("Relación entre las variables gender y survived")
```


Otro punto de vista. Survived como función de Embarked:

```{r}
ggplot(data=totalData[1:filas,],aes(x=embarked,fill=survived))+geom_bar(position="fill")+ylab("Frequència")+ggtitle("Survived como función de Embarked")
```

En la primera gráfica podemos observar fácilmente la cantidad de mujeres que viajaban respecto hombres y observar los que no sobrevivieron. Numéricamente el número de hombres y mujeres supervivientes es similar.

En la segunda gráfica de forma porcentual observamos los puertos de embarque y los porcentajes de supervivencia en función del puerto. Se podría trabajar el puerto C (Cherburgo) para ver de explicar la diferencia en los datos. Quizás porcentualmente embarcaron más mujeres o niños... ¿O gente de primera clase?

Obtenemos ahora una matriz de porcentajes de frecuencia.
Vemos, por ejemplo que la probabilidad de sobrevivir si se embarcó en "C" es de un 56.45%

```{r echo=TRUE, message=FALSE, warning=FALSE}
t<-table(totalData[1:filas,]$embarked,totalData[1:filas,]$survived)
for (i in 1:dim(t)[1]){
    t[i,]<-t[i,]/sum(t[i,])*100
}
t
```

Veamos ahora como en un mismo gráfico de frecuencias podemos trabajar con 3 variables: Embarked, Survived y class.

Mostramos el gráfico de embarcados por Pclass:

```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(data = totalData[1:filas,],aes(x=embarked,fill=survived))+geom_bar(position="fill")+facet_wrap(~class)+ggtitle("Pasajeros por clase, puerto de origen y relación con survived")
```

Aquí ya podemos extraer mucha información. Como propuesta de mejora se podría hacer un gráfico similar trabajando solo la clase. Habría que unificar toda la tripulación a una única categoría.

Comparamos ahora dos gráficos de frecuencias: Survived-SibSp y Survived-Parch

```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(data = totalData[1:filas,],aes(x=sibsp,fill=survived))+geom_bar()+ggtitle("Sobrevivir en función de tener a bordo cónyuges y/o hermanos")
ggplot(data = totalData[1:filas,],aes(x=parch,fill=survived))+geom_bar()+ggtitle("Sobrevivir en función de tener a bordo padres y/o hijos")
```

Vemos como la forma de estos dos gráficos es similar. Este hecho nos puede indicar presencia de correlaciones altas. Hecho previsible en función de la descripción de las variables.

Veamos un ejemplo de construcción de una variable nueva: Tamaño de familia

```{r echo=TRUE, message=FALSE, warning=FALSE}
totalData$FamilySize <- totalData$sibsp + totalData$parch +1;
totalData1<-totalData[1:filas,]
ggplot(data = totalData1[!is.na(totalData[1:filas,]$FamilySize),],aes(x=FamilySize,fill=survived))+geom_histogram(binwidth =1,position="fill")+ylab("Frecuencia")+ggtitle("Sobrevivir en función del número de familiares a bordo")
```
Se confirma el hecho de que los pasajeros viajaban mayoritariamente en familia. No podemos afirmar que el tamaño de la familia tuviera nada que ver con la posibilidad de sobrevivir pues nos tememos que estadísticamente el hecho de haber más familias de alrededor de cuatro miembros debería de ser habitual. Es un punto de partida para investigar más.

Veamos ahora dos gráficos que nos comparan los atributos Age y Survived.
Observamos como el parámetro position="fill" nos da la proporción acumulada de un atributo dentro de otro

```{r echo=TRUE, message=FALSE, warning=FALSE}
ggplot(data = totalData1[!(is.na(totalData[1:filas,]$age)),],aes(x=age,fill=survived))+geom_histogram(binwidth =3)+ggtitle("Sobrevivir en función de edad")
ggplot(data = totalData1[!is.na(totalData[1:filas,]$age),],aes(x=age,fill=survived))+geom_histogram(binwidth = 3,position="fill")+ylab("Frecuencia")+ggtitle("Sobrevivir en función de edad")
```


Observamos como el parámetro position="hijo" nos da la proporción acumulada de un atributo dentro de otro. Parece que los niños tuvieron más posibilidad de salvarse.

Vamos a probar si hay una correlación entre la edad del pasajero y el que pagó por el viaje

```{R}
# https://cran.r-project.org/web/packages/tidyverse/index.html
if (!require('tidyverse')) install.packages('tidyverse'); library('tidyverse')
cor.test(x = totalData$age, y = totalData$fare, method = "pearson")
ggplot(data = totalData, aes(x = age, y = log(fare))) + geom_point(color = "gray30") + geom_smooth(color = "firebrick") + theme_bw() +ggtitle("Correlación entre precio billete y edad")
```


Cómo podemos observar no parece haber correlación lineal entre la edad del pasajero y el precio del billete. El diagrama de dispersión tampoco apunta a ningún tipo de relación no lineal evidente.


## Conclusiones finales

Los datos tienen una calidad correcta y están mayoritariamente bien informados. Disponen de una variable de clase "survived" que los hace aptos para un clasificador.
A parte de la mayor supervivencia de mujeres y niños y de pasajeros de primera clase podemos observar la juventud de los pasajeros y la tripulación. Se observa también una gran cantidad de personas que viajaban en familia.

****
# Ejercicios
****

## Ejercicio 1:

Propon un proyecto completo de minería de datos. La organización de la respuesta tiene que coincidir con las fases típicas del ciclo de vida de un proyecto de minería de datos. *No hay que hacer las tareas de cada fase*. Para cada fase indica cuál es el objetivo de la fase y el producto que se obtendrá. Utiliza ejemplos de qué y como podrían ser las tareas. Si hay alguna característica que hace diferente el ciclo de vida de un proyecto de minería respecto a otros proyectos indícalo.


El proyecto propuesto es la creación de un algoritmo de recomendación de stocks para invertir en base a la similitud con otro. De esta manera, si un cliente está pensando en invertir en un stock determinado, se le pueda recomendar invertir en otros similares. Para determinar la similitud entre distintos stocks habría que tener en cuenta muchos factores como sector de la empresa, área geográfica, comportamiento histórico en el mercado, etc.

Un algoritmo de estas características podría ser interesante para entidades financieras como bancos o fondos de inversión, que puedan realizar recomendaciones a sus clientes para optimizar el retorno de sus inversiones.

### Definición de la tarea.

Tal y como se ha planteado el problema, el proyecto se podría categorizar como un proyecto de agregación o clustering. Se tomarán los datos o características de todos los stocks disponibles y se dividirán en grupos similares en base a éstos.

### Preparación de los datos.

En esta fase obtendríamos los datos con los que vamos a trabajar. En primer lugar, hay que definir los datos con los que vamos a realizar el clustering de stocks. Los datos tendrán que ser relevantes para medir la similitud entre activos; por ejemplo, el área geográfica de la empresa puede ser un factor relevante, pero no lo será el nombre del stock o el nombre de la calle de su sede social. Una propuesta que realizo es la siguiente:

* Sector de la empresa. Ej: energía, banca, automoción, tecnología, software… En principio, una sola empresa podría pertenecer a dos o más sectores. Por ejemplo, Apple podría estar dentro de los sectores tecnología, software y audiovisual o Tesla dentro de automoción y tecnología. La asignación de esta característica a los datos podría provenir en parte de alguna base de datos, pero también es posible que hubise que realizarla o completarla de forma manual.

* Área geográfica. Se podría dividir por país, pero quizás sería más conveniente por áreas geográficas con características similares como África, Norteamérica, Caribe, Unión Europea, Medio Oriente, etc, para no tratar con demasiadas categorías y que cada una de ellas tuviese una muestra lo suficientemente representativa.

* Comportamiento histórico del stock. No se trataría de un solo dato, sino de un conjunto de ellos que mostrasen el comportamiento histórico de su precio. Por ejemplo, se podría tomar la subida o bajada en el precio en los últimos 12 meses mes a mes, el de los últimos cinco años año a año o la desviación típica de la variación del precio como medida de la volatilidad del stock. También se podrían generar otros datos a partir del precio del stock más sofisticados que mejorasen el algoritmo de clustering, como el número de días al año que el precio ha tenido una EMA a 50 períodos con pendiente positiva o el número de veces que el RSI ha superado el nivel 80.

Las dos primeras características (sector y área geográfica) son de tipo categoría y nos tendríamos que asegurar que éstas son correctas y no hay errores de transcripción o duplicidades en las etiquetas (Unión Europea vs. UE, por ejemplo) ni datos incompletos (todo stock va a pertenecer a un área geográfica y, al menos, a un sector). Tanto el sector como el área los codificaríamos con one-hot encoding y, además, en el caso del sector podríamos tener a una empresa como perteneciente a más de una categoría.

Los datos de comportamiento histórico más básicos serían de tipo numérico. Para medir subidas o bajadas en los stocks, tanto mensuales como anuales, lo haremos en términos porcentuales. De esta manera se eliminan las diferencias de escala entre diferentes precios de stocks y las posibles diferentes monedas en las que se especifica cada uno. Además de esto, puede ser interesante hacer una normalización de los datos para que estén en le rango [-1,1] según el algoritmo final que se utilice. Para medir la volatilidad podría calcularse la desviación típica del cambio porcentual del precio diario. En este último dato, incluso, podría ser interesante hacer una discretización y convertirlo a dato categórico, dividiendo, por ejemplo, la volatilidad de los stocks en Muy Alta, Alta, Media, Baja o Muy baja. Si se decide finalmente incluir datos más sofisticados que muestren el comportamiento histórico, habrá que tratarlos y transformarlos según el caso.

### Construcción y evaluación del modelo.

Como hemos dicho, se trata de un proyecto de clustering, por lo que habría que realizar un proceso de búsqueda del mejor algoritmo para nuestra aplicación, o uno suficientemente bueno que cumpla con la calidad requerida y no sea computacionalmente demasiado exigente. En este caso, podríamos evaluar algoritmos como K-Means, DBSCAN o BIRCH.

Según el algoritmo elegido para la construcción del modelo, su evaluación puede ser distinta. Si trabajamos con k-means, por ejemplo, habrá que determinar el número de clusters que deseamos. En este caso, vamos a poder realizar una clasificación en grupos perfecta si aumentamos tanto el número de clústers como para que éste sea igual el número de stocks con los que trabajamos, pero es algo que no tendría sentido realizar, ya que el único stock recomendado sería él mismo. Habrá que elegir un número de clusters lo suficientemente pequeño como para que no se hagan recomendaciones de stocks muy diferentes al elegido, pero lo suficientemente grande como para que a cada clúster puedan pertenecer un puñado de empresas, que puedan ser recomendadas al cliente (2-5 empresas podría ser un número razonable). En el caso de k-means, por ejmplo, podríamos utilizar el método "elbow" para determinar el número óptimo de clusters.

### Despliegue.

No sería suficiente con hacer una primera clasificación de los stocks en el momento del despliegue, pues la similitud entre éstos es dinámica al cambiar el comportamiento del precio de cada uno de ellos, se pueden ir añadiendo nuevos stocks con el tiempo que habrá que asignar a algun cluster, y puede incluso cambiar el sector o sectores a los que pertenece cada uno (Apple con audiovisual, por ejemplo). Por lo tanto, habrá que realizar un entrenamiento periódico de modo que las recomendaciones esten actualizadas con la suficiente frecuencia; quizás para este caso una actualización semanal o incluso mensual podría ser suficiente, ya que tampoco estamos tratando con datos que vayan a cambiar con una frecuencia muy alta.


## Ejercicio 2:
A partir del juego de datos disponible en el siguiente enlace https://www.kaggle.com/rdoume/beerreviews , realiza las tareas previas a la generación de un modelo de minería de datos explicadas en los módulos "El proceso de minería de datos" y "Preprocesado de los datos y gestión de características". Puedes utilizar de referencia el ejemplo del Titánic.

En primer lugar, leemos los datos y echamos un vistazo a su estructura. 


```{r echo=TRUE, message=FALSE, warning=FALSE}
library(reticulate)
use_condaenv("r-reticulate")
```


```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv('beer_reviews.csv')
df.head(10)

```

```{python}
print(df.info())
```

Vemos como tenemos un dataset con 13 columnas y 1586614 registros que se corresponden a reviews hechas por consumidores de cerveza en la web BeerAdvocate. De entre los tipos de datos, podemos distinguir:

* Dos campos que definen la cervecera, uno de tipo entero (ID) y otro de tipo string (nombre).

* Tres campos que definen la cerveza: uno de tipo entero con el ID, otro de tipo string con el nombre de la cerveza y otro float con el porcentaje de alcohol de la cerveza.

* Un campo de tipo entero con una información de tiempo ('review_time'). Como no parece una información sobre tiempo legible, se interpreta que se trata en realidad de un Unix Timestamp, lo cual se comprueba convirtiendo los datos a fecha.

* Un campo string con el tipo de cerveza.

* Un campo string que identifica al usuario que ha realizado la review ('review_profilename')

* Cinco campos con la información del análisis realizado por el usuario. Estos son *aroma*, *apariencia*, *palatabilidad*, *sabor* y *puntuación general*. 


```{python}
df.isna().sum()/len(df)*100
```

Vemos el porcentaje de valores nulos que hay en cada una de las características y comprobamos que en ninguno de ellos es muy alto. Además, las valoraciones de los usuarios en las cinco categorías no tienen en ningún caso valores nulos. Sí lo tiene, por ejemplo, el campo de usuario que realiza la review, pero es un dato que no vamos a utilizar para el análisis, por lo que no haremos ninguna imputación y lo que haremos es eliminar esa columna. También faltan datos en el nombre de la cervecería; en este caso, y como se trata de muy pocos registros, los eliminamos del dataset. Por último, los datos que más faltan son los correspondientes al porcentaje de alcohol; aunque no son muchos, menos del 5%, no queremos perder esos registros y realizaremos una imputación con la mediana.

```{python}
df.drop('review_profilename', axis=1, inplace=True)
df.dropna(axis=0, subset=['brewery_name'], inplace=True)
df['beer_abv'].fillna(df['beer_abv'].median(), inplace=True)
```

Ahora convertimos los Unix timestamps en fechas legibles y con las que se podrá realizar análisis de forma mucho más fácil.

```{python}
df['review_time'] = pd.to_datetime(df['review_time'], unit='s')
df['review_time'].agg(['min','max'])
```

Se ha consultado el [siguiente enlace](https://stackoverflow.com/questions/19231871/convert-unix-time-to-readable-date-in-pandas-dataframe).

Vemos que los análisis más antiguos son de 1996 y los más recientes de 2012.

```{python}
df.info()
```

Vemos com ahora tenemos algún registro menos, pero no tenemos nungún valor nulo en el dataset. Hemos eliminado la columna de usuario que no nos aportaba información útil y vemos cómo el tipo de dato de la columna tiempo es efectivamente datetime en lugar de un entero.

Echamos un vistazo a las estadísticas básicas de los valores numéricos de las valoraciones y cantidad de alcohol, ya que las de los ID carece de sentido.

```{python}
# columnas con valores numéricos relevantes (no IDs)
num_cols = ['review_appearance', 'review_aroma', 'review_palate',
            'review_taste', 'review_overall', 'beer_abv']

# Columnas de valoraciones de usuarios
review_cols = ['review_appearance', 'review_aroma', 'review_palate',
               'review_taste', 'review_overall']

df[num_cols].describe().T
```

Vemos que review_appearance y review_overall tienen valores mínimos de 0 mientras que las otras valoraciones lo tienen de 1. Nos planteamos si puede ser un error.

```{python}
print(df['review_overall'].value_counts())
print(df['review_appearance'].value_counts())
```

Viendo el número tan bajo de apariciones del valor 0 en ambas y teniendo en cuenta que las posibles valoraciones van en múltipos de 0.5 puntos y no hay ninguna aparición de 0.5, parece confirmarse la sospecha de que se trata de un error.

Solucionamos el problema sustituyendo los 0s por 1s.

```{python}
df['review_overall'].replace(0, 1, inplace=True)
df['review_appearance'].replace(0, 1, inplace=True)
```


```{python}
df[num_cols].describe().T
```
Se puede ver ahora cómo todas las categorías tienen el mismo rango de valores. Vemos también que las valoraciones medias más altas son las de la apariencia y valoración general, y que la mayor variabilidad en respuestas se da en la valoración del sabor. 

Realizamos una inspección de la distribución de todas las valoraciones en la distintas categorías de forma visual.

```{python}
sns.set_theme()
sns.set_palette('Set2')
f, axes = plt.subplots(3, 2, sharex=True, sharey=True)
for i, feature in enumerate(review_cols):
  pfig = sns.countplot(data=df, x=feature, ax=axes[i%3, i//3])
f.tight_layout()
axes[2,1].set_axis_off()
plt.show()
```

Nos damos cuenta de que estas distribuciones son aproximadamente normales pero están algo sesgadas a la izquierda. En todos los casos el número más frecuente con el que se valora a la cerveza es un 4 y las notas más bajas rara vez son utilizadas.

En las estadísticas de los valores numéricos obtenidas anteriormente el pordentaje de alcohol medio es del 7% pero el mediano es del 6.5%, lo que quiere decir que habrá valores altos extremos que provoquen este aumento de la media, lo cual se puede comprobar en las mismas estadísticas con un valor máximo de 57.7%. Este valor en una cerveza parece demasiado alto. Vemos la distribución de porcentaje de alcohol de las cervezas con un histograma.

```{python}
plt.figure(figsize=(15,10))
pfig = sns.histplot(data=df, x='beer_abv', bins=30)
plt.xlabel('Porcentaje de alcohol')
plt.show()
```

```{python}
df.sort_values('beer_abv', ascending=False).head()
```

Efectivamente hay un pequeño número de cervezas con graduaciones muy altas. Inicialmente se piensa que es un error, pero tras comprobarse que efectivamente son [datos ciertos](https://www.brewdog.com/uk/worlds-strongest-beer), se dan por válidos.

Vamos a convertir esta característica numérica en una categórica dividiendo las cervezas en cuatro grupos distintos según la graduación:

* Baja: <5%

* Media: 5%-10%

* Alta: 10-15%

* Muy alta: >15%


```{python}
df['abv_range'] = pd.cut(df['beer_abv'], bins = [0, 5, 10, 15, 100], 
                         labels = ['Low', 'Medium', 'High', 'Very High'])
df['abv_range'].value_counts()
```

Podemos ver cuántas reviews se han realizado de cada una de las categorías de graduación en las que hemos dividido las muestras. Como era de esperar, la mayoría de las reviews son de cervezas con graduación media, entre el 5 y el 10%, y sólo un pequeño porcentaje son de graduación muy alta (>15%).

```{python}
df['beer_style'].unique()
```

Podemos ver que hay un gran número de estilos en los que se han dividido las cervezas. Vamos a crear una nueva variable de utilidad que nos indique el país de procedencia de dichos estilos. 


```{python}
import re

df["Country"] = df["beer_style"].str.extract(r'(American|English|Belgian|German)', \
                                            flags =  re.IGNORECASE)
df["Country"].fillna('Other', inplace=True)
df.info()

```

Nos hemos centrado en distinguir entre estilos de cerveza americanos, ingleses, belgas y alemanes, que parecen ser los más polulares. Los estilos que son de otros países, o de los que no disponemos de dicha información en el nombre, los englobamos dentro de *otros*.

Veamos la distribución del contenido de alcohol por países, haciendo uso de esta nueva característica que hemos creado.

```{python}
pfig = sns.catplot(data=df, x='beer_abv', y='Country', kind='boxen', aspect=2)
plt.show()
```

Con esta última gráfica podemos visualizar claramente la distribución de contenido de alcohol de las cervezas según el país de procedencia. Llama la atención el caso de alemania, que tiene casi todas las cervezas con una concentración de alcohol en torno al 5%, y la más baja de todos los países analizados. La mayor concentración media la tienen las belgas, con una mediana bastante alta del 9% aproximadamente. Las inglesas y americanas tienen una mayor dispersión, aunque estas últimas tienen una concentración media mayor.

Vamos ahora a comprobar qué cervezas son mejor valoradas (valoración general) según su graduación:


```{python}
plt.figure()
pfig = sns.catplot(data=df, x='abv_range', y='review_overall', kind='box', aspect=1.5)
plt.show()
```

Como podemos ver, la valoración mediana más alta las tienen las de graduación media y alta, mientras que las de graduación baja son las que presentan una mayor dispersión en los datos.


```{python}
plt.figure()
pfig = sns.catplot(data=df, x='abv_range', y='review_palate', kind='box', aspect=1.5)
plt.show()
```

Si miramos por ejemplo la valoración de palatibilidad, vemos como claramente las cervezas con graduación alta son las que mejor valoración obtienen. Nuevamente, las de baja graduación son las que mayor dispersión presentan.

Veamos qué cerveceras son las mejor valoradas en media.

```{python}
df.groupby('brewery_name')['review_overall'].agg(['mean', 'count'])\
  .sort_values('mean', ascending=False).head(10)
```

Comprobamos cómo hay unas cuantas cerveceras con la puntuación media máxima posible, pero también que tienen muy pocas valoraciones, lo que hace que la muestra no sea representativa.


```{python}
top = df.groupby('brewery_name')['review_overall'].agg(['mean', 'count'])
top[top['count'] > 100].sort_values('mean', ascending=False).head(10)
```

Si sólo miramos las que tienen un mínimo de 100 valoraciones, vemos cómo *The Alchemist* es la mejor valorada, con una puntuación media de casi 4.6. Esta sería una buena lista para hacer recomendaciones de marcas de cerveza.

Vamos a ver ahora cómo ha evolucionado la valoración del sabor a lo largo del tiempo.


```{python}
plt.figure()
by_year = df.groupby(df.review_time.dt.year)['review_taste'].mean()
pfig = sns.relplot(x=by_year.index , y=by_year, kind='line', aspect=1.5)
plt.show()
```

Vemos como hubo un pico muy alto de más de 4.1 en la valoración media del sabor de las cervezas en el año 2000, tras el cual se produjo una bajada brusca hasta los 3.7 y se fue recuperando lentamente y parece que se ha estabilizado ligeramente por encima del 3.8.

Vamos a ver ahora si las variables beer_avg (contenido de alcohol antes de categorizar) y review_aroma están correladas. En primer lugar, mediamos todas las valoraciones review_aroma pertenecientes a una misma cerveza (beer ID) y hacemos un scatter plot de ésta con review_aroma.


```{python}
plt.figure()
by_beer = df.groupby('beer_beerid')[['beer_abv', 'review_aroma']].mean()
pfig = sns.relplot(data=by_beer, x='beer_abv', y='review_aroma', alpha=0.2, aspect=1.5)
plt.show()
```

Podemos comprobar que sí existe cierta correlación positiva entre dichas variables, de forma que las cervezas con mayor graduación tienden a ser mejor valoradas en el apartado aroma. El mismo análisis podríamos hacer con el resto de variables.

Hacemos un análisis por estilos de cerveza tomando como variables la valoración general y el contenido medio de alcohol.

```{python}
by_style = df.groupby('beer_style')[['beer_abv', 'review_overall']].mean()
pfig = sns.jointplot(data=by_style, x="beer_abv", y="review_overall")
plt.show()
```

En el gráfico se puede observar cómo todos los estilos de cerveza de mayor graduación están muy bien valorados en la categoría general. También puede verse cómo la mayoría de los estilos están valorados en media en torno al 3.6-4 y que casi todos los estilos de cerveza tienen una graduación media entre el 5% y el 6%.

Dibujamos por último la matriz de correlación entre todas las variables numéricas.

```{python}
fig = plt.figure()
pfig = sns.heatmap(df[num_cols].corr(), linewidths=1, cmap="YlGnBu", annot=True)
fig.tight_layout()
plt.show()
```

Con esta matriz podemos ver claramente varias cosas:

* El contenido de alcohol es lo que menos influye en cualquera de las valoraciones, si bien en lo que más influye es en la valoración del aroma (ya habíamos visto una correlación positiva anteriormente) y en la que menos en la apariencia.

* Como es de esperar, las valoraciones realizadas por los usuarios están bastante correladas entre si. Esto quiere decir que una buena cerveza tenderá a estar bien valorada en sus diferentes aspectos y una mala también lo será en todos ellos.

* Las valoraciones *general* y *sabor* son las más correladas, lo que quiere decir que,  probablemente, los usuarios han valorado esta característica con más peso que el resto a la hora de hacer la valoración general, lo cual tiene sentido y no llama la atención.

* Lo contrario ocurre con *general* y *apariencia*, que son las menos correladas. Parece que la apariencia es lo menos importante para los usuarios a la hora de realizar una valoración general de la cerveza.


Supongamos ahora que queremos predecir la valoración general de una cerveza en función de la valoración realizada en cada uno de sus aspectos y otras variables. Teniendo en cuenta que la correlación entre las valoraciones es muy alta, es probable pensar que podamos reducir el número de variables sin perder apenas información. Para hacerlo, vamos a realizar un análisis PCA (Principal component Analysis) con el mismo número de componentes que tenemos actualmente, pero con la particularidad de que ahora son una combinación lineal de las anteriores de modo que la primera componente explique la máxima variabilidad y así sucesivamente, hasta que la última contenga la mínima información.

```{python}
from sklearn.decomposition import PCA

factors = ['review_appearance', 'review_aroma', 'review_palate', 'review_taste']

# Deberíamos realizar una normalización por la diferencia de los datos para que
# estuviesen en la misma escala, pero en este caso ya lo están, así que nos 
# ahorramos ese paso

n_pca = 4

pca = PCA(n_components=n_pca)
pca_val = pca.fit_transform(df[factors])
pca_df = pd.DataFrame(pca_val, columns=['pca1', 'pca2', 'pca3', 'pca4'])

# Unimos el DatFrame original con las dos nuevas columnas
final_df = pd.concat([df, pca_df], axis=1)
```


```{python}
final_df.info()
```

Hemos creado cuatro nuevas columnas en el dataframe con las componentes PCA. Ahora estas componentes no se corresponden directamente con ninguna de las valoraciones, sino que todas ellas son una combinación lineal de ellas.

Realizamos ahora un análisis sobre la información útil que nos aporta cada una de estas nuevas componentes para evaluar con cuántas de ellas nos hemos de quedar para no perder mucha información.


```{python}
pca_var = pca.explained_variance_ratio_
plt.figure()
pfig = plt.bar(x=range(n_pca), height=pca_var)
plt.xlabel('PCA component')
plt.ylabel('Explained variance ratio')
plt.xticks(list(range(n_pca)))
for i, v in enumerate(pca_var):
    plt.gca().text(i-0.09, v+0.01, f'{v:.2f}', fontweight='bold')
plt.show()
```

Se ha consultado el siguiente [enlace](https://stackoverflow.com/questions/30228069/how-to-display-the-value-of-the-bar-on-each-bar-with-pyplot-barh).

En la gráfica anterior se puede ver cómo con tan solo la primera de las componentes PCA tenemos el 73% de la información que tendríamos con las cuatro componentes originales, que ya es un número muy importante. También se puede apreciar que la última de las componentes apenas aporta información, por lo que seguramente la decisión estaría entre tomar sólo la primera, o tomar las tres primeras, según fuesen los requerimientos de diseño del modelo.

Con las componentes seleccionadas, además de otras variables (tipo de cerveza o cantidad de alcohol) podríamos crear un modelo para predecir la valoración general con bastante precisión, si ese fuese nuestro objetivo.

***

## Conclusiones Ejercicio 2
Se han importado los datos y realizado una inspección básica de éstos, se ha lidiado justificadamente con los valores nulos presentes, arreglado problemas con valores erróneos, discretizado una variable, cambiado la escala, cambiado el tipo a otra (datetime) y generado una nueva útil para el análisis (country). También se ha hecho una inspección visual de los datos, viendo la distribución de las variables numéricas, la corelación entre variables y dando respuesta a diferentes preguntas. También se ha realizado un ejemplo de transformación previa al modelado, en este caso un análisis PCA para reducir su complejidad.

Tras este análisis tenemos un mejor conocimiento de los datos y los hemos preparado para poder aplicar un modelo de minería de datos. Las conclusiones extraídas del análisis han sido coherentes y no se han encontrado incongruencias.




***
# Criterios de evaluación
***

Ejercicio 1

Concepto y peso en la nota final

El objetivo del proyecto está correctamente definido con suficiente concreción y se puede resolver con técnicas de minería de datos. 15%

Las fases del ciclo de vida están bien expresadas. Los ejemplos son clarificadores. Se justifica y argumenta de las decisiones que se han tomado. 20%

Ejercicio 2

Se carga la base de datos, se visualiza su estructura y se explican los hechos básicos de los datos. 5%

Se estudia si existen atributos vacíos o en diferentes escalas que haya que normalizar. Si es el caso se adoptan medidas para tratar estos atributos. Se construye un nueva variable útil a partir de las existentes. Se discretiza algún atributo. 20%

Se analizan los datos de forma visual y extraen conclusiones tangibles. Hay que elaborar un discurso coherente y con conclusiones claras. 30%

Se trata en profundidad alguno otro aspecto respecto a los datos presentado en los módulos "Preprocesado de los datos y gestión de características" 10%
